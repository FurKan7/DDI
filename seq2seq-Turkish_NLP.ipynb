{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    else:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 324 sentence pairs\n",
      "Trimmed to 324 sentence pairs\n",
      "Indexing words...\n",
      "['o guzel bir yenilik volurdu .', 'o guzel bir inovasyon olurdu .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('tur', 'tur', True)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "#     print('var =', var)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = GeneralAttn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    #def score(self, hidden, encoder_output):\n",
    "        \n",
    "     #   if self.method == 'dot':\n",
    "     #       energy = hidden.dot(encoder_output)\n",
    "            #energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "      #      return energy\n",
    "        \n",
    "       # elif self.method == 'general':\n",
    "        #    energy = self.attn(encoder_output)\n",
    "         #   energy = hidden.dot(energy)\n",
    "          #  return energy\n",
    "        \n",
    "       # elif self.method == 'concat':\n",
    "       #     energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "        #    energy = self.v.dot(energy)\n",
    "         #   return energy\n",
    "        \n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy =torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = torch.dot(self.v.view(-1), energy.view(-1))\n",
    "        return energy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(10, 10, num_layers=2)\n",
      ")\n",
      "AttnDecoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(20, 10, num_layers=2, dropout=0.1)\n",
      "  (out): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (attn): Attn(\n",
      "    (attn): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "encoder_test = EncoderRNN(10, 10, 2)\n",
    "decoder_test = AttnDecoderRNN('general', 10, 10, 2)\n",
    "print(encoder_test)\n",
    "print(decoder_test)\n",
    "\n",
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "word_inputs = Variable(torch.LongTensor([1, 2, 3]))\n",
    "decoder_attns = torch.zeros(1, 3, 3)\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder_context = Variable(torch.zeros(1, decoder_test.hidden_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "for i in range(3):\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(word_inputs[i], decoder_context, decoder_hidden, encoder_outputs)\n",
    "    print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n",
    "    decoder_attns[0, i] = decoder_attn.squeeze(0).cpu().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di] # Next target is next input\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use network's own prediction as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            \n",
    "            # Get most likely word index (highest value) from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            \n",
    "            decoder_input = Variable(torch.LongTensor([[ni]])) # Chosen word is next input\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 4\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 100000\n",
    "plot_every = 200\n",
    "print_every = 1000\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually train, we call the train function many times, printing a summary as we go.\n",
    "\n",
    "*Note:* If you run this notebook you can train, interrupt the kernel, evaluate, and continue training later. You can comment out the lines above where the encoder and decoder are initialized (so they aren't reset) or simply run the notebook starting from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 36s (- 59m 32s) (1000 1%) 4.4055\n",
      "1m 13s (- 59m 47s) (2000 2%) 3.3103\n",
      "1m 51s (- 59m 58s) (3000 3%) 2.4727\n",
      "2m 30s (- 60m 3s) (4000 4%) 1.8812\n",
      "3m 8s (- 59m 49s) (5000 5%) 1.3439\n",
      "3m 48s (- 59m 32s) (6000 6%) 0.9250\n",
      "4m 27s (- 59m 8s) (7000 7%) 0.6591\n",
      "5m 6s (- 58m 42s) (8000 8%) 0.4505\n",
      "5m 44s (- 58m 8s) (9000 9%) 0.2990\n",
      "6m 24s (- 57m 44s) (10000 10%) 0.1991\n",
      "7m 4s (- 57m 12s) (11000 11%) 0.1203\n",
      "7m 43s (- 56m 39s) (12000 12%) 0.0864\n",
      "8m 22s (- 56m 4s) (13000 13%) 0.0475\n",
      "9m 1s (- 55m 23s) (14000 14%) 0.0407\n",
      "9m 40s (- 54m 49s) (15000 15%) 0.0472\n",
      "10m 19s (- 54m 13s) (16000 16%) 0.0222\n",
      "10m 58s (- 53m 33s) (17000 17%) 0.0142\n",
      "11m 36s (- 52m 52s) (18000 18%) 0.0226\n",
      "12m 15s (- 52m 14s) (19000 19%) 0.0331\n",
      "12m 54s (- 51m 38s) (20000 20%) 0.0167\n",
      "13m 33s (- 51m 0s) (21000 21%) 0.0377\n",
      "14m 12s (- 50m 22s) (22000 22%) 0.0063\n",
      "14m 50s (- 49m 42s) (23000 23%) 0.0451\n",
      "15m 30s (- 49m 7s) (24000 24%) 0.0233\n",
      "16m 10s (- 48m 30s) (25000 25%) 0.0031\n",
      "16m 48s (- 47m 51s) (26000 26%) 0.0056\n",
      "17m 27s (- 47m 12s) (27000 27%) 0.0348\n",
      "18m 6s (- 46m 33s) (28000 28%) 0.0222\n",
      "18m 44s (- 45m 53s) (29000 28%) 0.0038\n",
      "19m 23s (- 45m 15s) (30000 30%) 0.0216\n",
      "20m 2s (- 44m 36s) (31000 31%) 0.0064\n",
      "20m 40s (- 43m 56s) (32000 32%) 0.0130\n",
      "21m 20s (- 43m 18s) (33000 33%) 0.0058\n",
      "21m 59s (- 42m 40s) (34000 34%) 0.0222\n",
      "22m 37s (- 42m 1s) (35000 35%) 0.0255\n",
      "23m 16s (- 41m 22s) (36000 36%) 0.0453\n",
      "23m 54s (- 40m 42s) (37000 37%) 0.0239\n",
      "24m 33s (- 40m 3s) (38000 38%) 0.0045\n",
      "25m 12s (- 39m 25s) (39000 39%) 0.0131\n",
      "25m 51s (- 38m 46s) (40000 40%) 0.0264\n",
      "26m 29s (- 38m 7s) (41000 41%) 0.0126\n",
      "27m 8s (- 37m 29s) (42000 42%) 0.0316\n",
      "27m 47s (- 36m 50s) (43000 43%) 0.0205\n",
      "28m 26s (- 36m 11s) (44000 44%) 0.0002\n",
      "29m 4s (- 35m 32s) (45000 45%) 0.0001\n",
      "29m 43s (- 34m 53s) (46000 46%) 0.0001\n",
      "30m 21s (- 34m 14s) (47000 47%) 0.0000\n",
      "31m 0s (- 33m 35s) (48000 48%) 0.0000\n",
      "31m 39s (- 32m 57s) (49000 49%) 0.0000\n",
      "32m 18s (- 32m 18s) (50000 50%) 0.0000\n",
      "32m 57s (- 31m 40s) (51000 51%) 0.0000\n",
      "33m 37s (- 31m 2s) (52000 52%) 0.0000\n",
      "34m 16s (- 30m 24s) (53000 53%) 0.0000\n",
      "34m 56s (- 29m 45s) (54000 54%) 0.0000\n",
      "35m 36s (- 29m 8s) (55000 55%) 0.0000\n",
      "36m 16s (- 28m 30s) (56000 56%) 0.0000\n",
      "36m 56s (- 27m 51s) (57000 56%) 0.0000\n",
      "37m 36s (- 27m 13s) (58000 57%) 0.0720\n",
      "38m 14s (- 26m 34s) (59000 59%) 0.0318\n",
      "38m 54s (- 25m 56s) (60000 60%) 0.0075\n",
      "39m 35s (- 25m 18s) (61000 61%) 0.0053\n",
      "40m 15s (- 24m 40s) (62000 62%) 0.0130\n",
      "40m 54s (- 24m 1s) (63000 63%) 0.0062\n",
      "41m 35s (- 23m 23s) (64000 64%) 0.0002\n",
      "42m 14s (- 22m 44s) (65000 65%) 0.0000\n",
      "42m 53s (- 22m 5s) (66000 66%) 0.0000\n",
      "43m 32s (- 21m 26s) (67000 67%) 0.0000\n",
      "44m 11s (- 20m 47s) (68000 68%) 0.0000\n",
      "44m 50s (- 20m 8s) (69000 69%) 0.0000\n",
      "45m 29s (- 19m 29s) (70000 70%) 0.0000\n",
      "46m 10s (- 18m 51s) (71000 71%) 0.0000\n",
      "46m 49s (- 18m 12s) (72000 72%) 0.0000\n",
      "47m 29s (- 17m 33s) (73000 73%) 0.0000\n",
      "48m 9s (- 16m 55s) (74000 74%) 0.0000\n",
      "48m 48s (- 16m 16s) (75000 75%) 0.0000\n",
      "49m 28s (- 15m 37s) (76000 76%) 0.0000\n",
      "50m 7s (- 14m 58s) (77000 77%) 0.0000\n",
      "50m 47s (- 14m 19s) (78000 78%) 0.0657\n",
      "51m 27s (- 13m 40s) (79000 79%) 0.0333\n",
      "52m 7s (- 13m 1s) (80000 80%) 0.0034\n",
      "52m 48s (- 12m 23s) (81000 81%) 0.0045\n",
      "53m 28s (- 11m 44s) (82000 82%) 0.0029\n",
      "54m 8s (- 11m 5s) (83000 83%) 0.0064\n",
      "54m 47s (- 10m 26s) (84000 84%) 0.0000\n",
      "55m 27s (- 9m 47s) (85000 85%) 0.0000\n",
      "56m 6s (- 9m 7s) (86000 86%) 0.0000\n",
      "56m 45s (- 8m 28s) (87000 87%) 0.0000\n",
      "57m 24s (- 7m 49s) (88000 88%) 0.0000\n",
      "58m 3s (- 7m 10s) (89000 89%) 0.0000\n",
      "58m 41s (- 6m 31s) (90000 90%) 0.0000\n",
      "59m 20s (- 5m 52s) (91000 91%) 0.0000\n",
      "59m 58s (- 5m 12s) (92000 92%) 0.0000\n",
      "60m 37s (- 4m 33s) (93000 93%) 0.0015\n",
      "61m 16s (- 3m 54s) (94000 94%) 0.0757\n",
      "61m 55s (- 3m 15s) (95000 95%) 0.0004\n",
      "62m 34s (- 2m 36s) (96000 96%) 0.0011\n",
      "63m 12s (- 1m 57s) (97000 97%) 0.0000\n",
      "63m 51s (- 1m 18s) (98000 98%) 0.0000\n",
      "64m 30s (- 0m 39s) (99000 99%) 0.0000\n",
      "65m 9s (- 0m 0s) (100000 100%) 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    training_pair = variables_from_pair(random.choice(pairs))\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[int(ni)])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> tanıtımın hazırlıgına bir hafta onceden basladılar .\n",
      "= lansmanın hazırlıgına bir hafta onceden basladılar .\n",
      "< lansmanın hazırlıgına bir hafta onceden basladılar . <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/furkan/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
